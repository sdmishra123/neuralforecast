{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1474011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.tsmixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1db630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2febcac",
   "metadata": {},
   "source": [
    "# TSMixer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce83d05",
   "metadata": {},
   "source": [
    "Time-Series Mixer (`TSMixer`), a novel architecture designed by stacking multi-layer perceptrons (`MLPs`) that focuses on mixing time and feature dimensions to make better predictions.It is designed for time series forecasting, specifically in scenarios where the data is multivariate and exhibits complex dynamics. \n",
    "\n",
    "(`TSMixer`) presents a promising approach to time series forecasting by effectively leveraging both temporal and cross-variate information through a simple yet powerful architecture.\n",
    "\n",
    "(`TSMixer`) demonstrates superior performance compared to the state-of-the-art alternatives on the challenging and large scale M5 benchmark\n",
    "\n",
    "**References**<br>\n",
    "-[Si-An Chen, Chun-Liang Li, Nathanael C. Yoder, Sercan Ö. Arık,Tomas Pfister. \"TSMixer: An All-MLP Architecture for Time Series Forecasting\".](https://arxiv.org/pdf/2303.06053.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f079989",
   "metadata": {},
   "source": [
    "![Figure 1. TSMixer Model Architecture.](imgs_models/tsmixer.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd45430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import neuralforecast\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import neuralforecast\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_multivariate import BaseMultivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TransposeLayer(nn.Module):\n",
    "    def __init__(self, dim0, dim1):\n",
    "        super(TransposeLayer, self).__init__()\n",
    "        self.dim0 = dim0\n",
    "        self.dim1 = dim1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.transpose(x, self.dim0, self.dim1)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, inputs, norm_type='L', activation=nn.ReLU, dropout=0., ff_dim=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        # inputs  - # [Batch, Input Length, Channel] [0,1,2]\n",
    "        \n",
    "        if norm_type == 'L':\n",
    "            self.norm = nn.LayerNorm\n",
    "        else:\n",
    "            self.norm = nn.BatchNorm1d\n",
    "\n",
    "\n",
    "        # print(inputs[-1])\n",
    "        # print(ff_dim)\n",
    "        \n",
    "        # Temporal Linear\n",
    "        self.temporal_linear = nn.Sequential(\n",
    "            self.norm(1),         \n",
    "            TransposeLayer(1, 2), # [Batch, Channel, Input Length] \n",
    "            nn.Linear(inputs[-1], inputs[-1], bias=False),\n",
    "            activation(), \n",
    "            TransposeLayer(1, 2),\n",
    "            nn.Dropout(dropout)\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Feature Linear\n",
    "        self.feature_linear = nn.Sequential(\n",
    "            self.norm(1),\n",
    "            nn.Linear(inputs[-1],ff_dim, bias=False),\n",
    "            activation(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, inputs[-1], bias=False),\n",
    "            activation(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Temporal Linear\n",
    "        x = self.temporal_linear(inputs)\n",
    "        res = x + inputs\n",
    "\n",
    "        # Feature Linear\n",
    "        x = self.feature_linear(res)\n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSMixer(BaseMultivariate):\n",
    "    \"\"\" TSMixer\n",
    "    \n",
    "    Basic TSMixer Model architecture.    \n",
    "    This deep neural network model builds upon the idea that linear models are effective for capturing time dependencies. \n",
    "    The TSMixer architecture combines linear models with non-linearities, stacking them in alternating layers for time and feature domains. \n",
    "    This approach leverages both temporal patterns and covariate information. \n",
    "    The model includes time-mixing and feature-mixing MLPs, temporal projection, residual connections, and 2D normalization.\n",
    "    It is trained using ADAM stochastic gradient descent.\n",
    "    The network accepts static, historic and future exogenous data, flattens the inputs and learns fully connected relationships against the target variable.\n",
    "    \n",
    "    **Parameters:**<br> \n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br> \n",
    "    `n_series`: int, number of time-series.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `n_series`:int, the number of time series in the multivariate time series data. <br>\n",
    "    `batch_size`: int=32, number of different series in each batch.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `hidden_size`: int, number of units for each layer of the MLP.<br>\n",
    "    `n_layers`: int, number of layers for the MLP.<br>\n",
    "    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `alias`: str, optional, Custom name of the model.<br>    \n",
    "    `**trainer_kwargs`: int, keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "    \n",
    "    `Additional parameters for TSMixer model:-  \n",
    "    `activation`: str, activation from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid'].<br> \n",
    "    `n_block`: int , number of mixer layers/residual blocks.<br>\n",
    "    `dropout`: float=0., dropout regularization.<br>\n",
    "    `ff_dim`:int, number of features or characteristics the model should consider when processing information about the data. <br>    \n",
    "    `target_slice`: list[start,end] the target_slice parameter is used to customize the predicted sequence the model should focus on. When target_slice=None - In this case, the model predicts the entire sequence.<br>   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'windows'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h,                          # prediction length                 \n",
    "                 input_size,                 # Autoregressive terms\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 max_steps: int = 1000,\n",
    "                 val_check_steps: int = 100,\n",
    "                 step_size: int = 1,\n",
    "                 num_lr_decays: int = 0,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 # hidden_size: int = 1,\n",
    "                 # n_layers: int = 1,\n",
    "                 scaler_type: str = 'robust',\n",
    "                 futr_exog_list=None,\n",
    "                 hist_exog_list=None,\n",
    "                 stat_exog_list=None,\n",
    "                 num_workers_loader: int = 0,\n",
    "                 drop_last_loader: bool = False,\n",
    "                 random_seed: int =1, \n",
    "                 alias=None,\n",
    "                 \n",
    "                 ###### Specific to TSMixer                 \n",
    "                 n_series:int =None,        # Number of input channels\n",
    "                 batch_size: int = 32,      # The input batch size\n",
    "                 # activation: str = 'ReLU',  # Activation required to normalize the MLP \n",
    "                 n_block: int =None,        # Number of residual blocks\n",
    "                 dropout:float =.01,        # Droput for MLP layers\n",
    "                 ff_dim:int = None,         # Feature dimensions for each layers\n",
    "                 target_slice:list =None,   # Prediction window user wnats to analyze\n",
    "                 ######\n",
    "                 **trainer_kwargs\n",
    "                ):\n",
    "        \n",
    "  \n",
    "        super(TSMixer, self).__init__(\n",
    "                                      h=h,\n",
    "                                      input_size=input_size,\n",
    "                                      loss=loss,\n",
    "                                      valid_loss=valid_loss,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      max_steps=max_steps,\n",
    "                                      val_check_steps=val_check_steps,                                      \n",
    "                                      step_size=step_size,\n",
    "                                      num_lr_decays=num_lr_decays,\n",
    "                                      early_stop_patience_steps=early_stop_patience_steps,\n",
    "                                      # hidden_size=hidden_size,\n",
    "                                      # n_layers=n_layers,\n",
    "                                      scaler_type=scaler_type,\n",
    "                                      futr_exog_list=futr_exog_list,\n",
    "                                      hist_exog_list=hist_exog_list,\n",
    "                                      stat_exog_list=stat_exog_list,\n",
    "                                      num_workers_loader=num_workers_loader,\n",
    "                                      drop_last_loader=drop_last_loader,\n",
    "                                      random_seed=random_seed, \n",
    "                                      alias=alias,\n",
    "            \n",
    "                                      ##### Specific to TSMixer                                       \n",
    "                                      # activation = activation,\n",
    "                                      n_block=n_block,\n",
    "                                      dropout=dropout,\n",
    "                                      ff_dim=ff_dim,\n",
    "                                      target_slice=target_slice,\n",
    "                                      n_series=n_series,\n",
    "                                      batch_size=batch_size,\n",
    "                                      #####\n",
    "                                      **trainer_kwargs\n",
    "                                     )\n",
    "        \n",
    "\n",
    "        \n",
    "        # self.h = h\n",
    "        # self.input_size = input_size\n",
    "        self.n_series = n_series\n",
    "\n",
    "        #Parameters specific to TSMixer.\n",
    "        # self.activation = activation\n",
    "        self.n_block = n_block\n",
    "        self.dropout = dropout\n",
    "        self.ff_dim = ff_dim\n",
    "        self.target_slice = target_slice\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create TSMixer-specific modules with learnable parameters        \n",
    "        input_shape = (batch_size, input_size, n_series)  # Assuming batch_size, input_size, and n_series are user inputs\n",
    "   \n",
    "        self.res_blocks = nn.ModuleList([ResBlock(inputs = input_shape, dropout=dropout, ff_dim=ff_dim) for _ in range(self.n_block)])\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            TransposeLayer(1, 2),\n",
    "            nn.Linear(input_shape[-1],h),\n",
    "            TransposeLayer(1, 2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, windows_batch):\n",
    "        \n",
    "        x = windows_batch['insample_y']\n",
    "        # insample_mask = windows_batch['insample_mask']\n",
    "        # futr_exog = windows_batch['futr_exog']\n",
    "        # hist_exog = windows_batch['hist_exog']\n",
    "        # stat_exog = windows_batch['stat_exog']\n",
    "        \n",
    "        input_shape = x.shape[1:]\n",
    "        batch_size = x.size(0)  # Get the dynamic batch size\n",
    "\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x.view(batch_size,*input_shape))  # Reshape to include batch_size\n",
    "\n",
    "        if self.target_slice:\n",
    "            x = x[:, :, self.target_slice]\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        y_pred = x.reshape(x.size(0), self.h, self.loss.outputsize_multiplier)\n",
    "        # y_pred = x.view(batch_size, self.h, self.loss.outputsize_multiplier)\n",
    "        y_pred = self.loss.domain_map(y_pred)\n",
    "\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
