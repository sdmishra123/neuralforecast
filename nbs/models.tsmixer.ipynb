{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1474011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.tsmixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1db630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2febcac",
   "metadata": {},
   "source": [
    "# TSMixer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce83d05",
   "metadata": {},
   "source": [
    "Time-Series Mixer (`TSMixer`), a novel architecture designed by stacking multi-layer perceptrons (`MLPs`) that focuses on mixing time and feature dimensions to make better predictions.It is designed for time series forecasting, specifically in scenarios where the data is multivariate and exhibits complex dynamics. \n",
    "\n",
    "(`TSMixer`) presents a promising approach to time series forecasting by effectively leveraging both temporal and cross-variate information through a simple yet powerful architecture.\n",
    "\n",
    "(`TSMixer`) demonstrates superior performance compared to the state-of-the-art alternatives on the challenging and large scale M5 benchmark\n",
    "\n",
    "**References**<br>\n",
    "-[Si-An Chen, Chun-Liang Li, Nathanael C. Yoder, Sercan Ö. Arık,Tomas Pfister. \"TSMixer: An All-MLP Architecture for Time Series Forecasting\".](https://arxiv.org/pdf/2303.06053.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f079989",
   "metadata": {},
   "source": [
    "![Figure 1. TSMixer Model Architecture.](imgs_models/tsmixer.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd45430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import neuralforecast\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import neuralforecast\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_multivariate import BaseMultivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TransposeLayer(nn.Module):\n",
    "    def __init__(self, dim0, dim1):\n",
    "        super(TransposeLayer, self).__init__()\n",
    "        self.dim0 = dim0\n",
    "        self.dim1 = dim1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.transpose(x, self.dim0, self.dim1)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, inputs, activation=nn.ReLU, dropout=0., ff_dim=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm\n",
    "        \n",
    "        # Temporal Linear\n",
    "        self.temporal_linear = nn.Sequential(\n",
    "            self.norm(inputs, eps=1e-5),  \n",
    "            TransposeLayer(1,2), # [Batch, Channel, Input Length]                                               \n",
    "            nn.Linear(inputs[-1],inputs[-1], bias=False),\n",
    "            activation(), \n",
    "            TransposeLayer(1, 2),\n",
    "            nn.Dropout(dropout)\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Feature Linear\n",
    "        self.feature_linear = nn.Sequential(\n",
    "            self.norm(inputs[1:], eps=1e-5),\n",
    "            nn.Linear(inputs[-1],ff_dim, bias=False),  # [Batch, Input Length, FF_Dim]\n",
    "            activation(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, inputs[-1], bias=False),  # [Batch, Input Length, Channel]\n",
    "            activation(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Temporal Linear\n",
    "        x = self.temporal_linear(inputs)\n",
    "        res = x + inputs\n",
    "\n",
    "        # Feature Linear\n",
    "        x = self.feature_linear(res)\n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSMixer(BaseMultivariate):\n",
    "    \"\"\" TSMixer\n",
    "    \n",
    "    Basic TSMixer Model architecture.    \n",
    "    This deep neural network model builds upon the idea that linear models are effective for capturing time dependencies. \n",
    "    The TSMixer architecture combines linear models with non-linearities, stacking them in alternating layers for time and feature domains. \n",
    "    This approach leverages both temporal patterns and covariate information. \n",
    "    The model includes time-mixing and feature-mixing MLPs, temporal projection, residual connections, and 2D normalization.\n",
    "    It is trained using ADAM stochastic gradient descent.\n",
    "    The network accepts static, historic and future exogenous data, flattens the inputs and learns fully connected relationships against the target variable.\n",
    "    \n",
    "    **Parameters:**<br> \n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br> \n",
    "    `n_series`: int, number of time-series.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `n_series`:int, the number of time series in the multivariate time series data. <br>\n",
    "    `batch_size`: int=32, number of different series in each batch.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `hidden_size`: int, number of units for each layer of the MLP.<br>\n",
    "    `n_layers`: int, number of layers for the MLP.<br>\n",
    "    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `alias`: str, optional, Custom name of the model.<br>    \n",
    "    `**trainer_kwargs`: int, keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "    \n",
    "    `Additional parameters for TSMixer model:-  \n",
    "    `activation`: str, activation from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid'].<br> \n",
    "    `n_block`: int , number of mixer layers/residual blocks.<br>\n",
    "    `dropout`: float=0., dropout regularization.<br>\n",
    "    `ff_dim`:int, number of features or characteristics the model should consider when processing information about the data. <br>    \n",
    "    `target_slice`: slice[start,end] the target_slice parameter is used to customize the predicted sequence the model should focus on. When target_slice=None - In this case, the model predicts the entire sequence.<br>   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'windows'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h,                                           \n",
    "                 input_size,                 \n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 max_steps: int = 1000,\n",
    "                 val_check_steps: int = 100,\n",
    "                 step_size: int = 1,\n",
    "                 num_lr_decays: int = 0,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 scaler_type: str = 'robust',\n",
    "                 futr_exog_list=None,\n",
    "                 hist_exog_list=None,\n",
    "                 stat_exog_list=None,\n",
    "                 num_workers_loader: int = 0,\n",
    "                 drop_last_loader: bool = False,\n",
    "                 random_seed: int =1, \n",
    "                 alias=None,\n",
    "                 \n",
    "                 ###### Specific to TSMixer                 \n",
    "                 n_series:int =None,        \n",
    "                 batch_size: int = 32,      \n",
    "                 n_block: int =None,        \n",
    "                 dropout:float =.01,      \n",
    "                 ff_dim:int = None,         \n",
    "                 target_slice:list =None,   \n",
    "                 ######\n",
    "                 **trainer_kwargs\n",
    "                ):\n",
    "        \n",
    "  \n",
    "        super(TSMixer, self).__init__(\n",
    "                                      h=h,\n",
    "                                      input_size=input_size,\n",
    "                                      loss=loss,\n",
    "                                      valid_loss=valid_loss,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      max_steps=max_steps,\n",
    "                                      val_check_steps=val_check_steps,                                      \n",
    "                                      step_size=step_size,\n",
    "                                      num_lr_decays=num_lr_decays,\n",
    "                                      early_stop_patience_steps=early_stop_patience_steps,\n",
    "                                      scaler_type=scaler_type,\n",
    "                                      futr_exog_list=futr_exog_list,\n",
    "                                      hist_exog_list=hist_exog_list,\n",
    "                                      stat_exog_list=stat_exog_list,\n",
    "                                      num_workers_loader=num_workers_loader,\n",
    "                                      drop_last_loader=drop_last_loader,\n",
    "                                      random_seed=random_seed, \n",
    "                                      alias=alias,\n",
    "                                      n_series=n_series,\n",
    "                                      batch_size=batch_size,\n",
    "                                      #####\n",
    "                                      **trainer_kwargs\n",
    "                                     )\n",
    "\n",
    "        #Parameters specific to TSMixer.\n",
    "        self.batch_size = batch_size\n",
    "        self.n_block = n_block\n",
    "        self.dropout = dropout\n",
    "        self.ff_dim = ff_dim\n",
    "        self.target_slice = target_slice  \n",
    "        self.n_series = n_series\n",
    "        \n",
    "        # Create TSMixer-specific modules with learnable parameters        \n",
    "        input_shape = (input_size, n_series)  \n",
    "        \n",
    "        self.res_blocks = nn.ModuleList([ResBlock(inputs = input_shape, dropout=dropout, ff_dim=ff_dim) for _ in range(self.n_block)])\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            TransposeLayer(1,2),\n",
    "            nn.Linear(input_shape[-1],input_shape[-1]),            \n",
    "            TransposeLayer(1,2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, windows_batch):\n",
    "\n",
    "        insample_y = windows_batch['insample_y']\n",
    "        x = insample_y\n",
    "        \n",
    "        X = x.unsqueeze(1).permute(0,1,2,3).contiguous()\n",
    "        \n",
    "        for res_block in self.res_blocks:            \n",
    "            x = res_block(X)\n",
    "\n",
    "        if self.target_slice is not None:\n",
    "            x = x[:, :, :, self.target_slice]\n",
    "    \n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        x = x.view(x.size(0), self.n_series,-1,self.h).contiguous() \n",
    "        \n",
    "        x = TransposeLayer(1, 2)(x)\n",
    "          \n",
    "        y_pred = x.reshape(x.size(0), self.h,self.n_series)\n",
    "        \n",
    "        y_pred = self.loss.domain_map(y_pred)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc686d3f-f502-4b03-8521-d99259b75960",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc94324-592a-435e-aca8-241aaa928dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhavy\\AppData\\Local\\Temp\\ipykernel_11420\\643683725.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was too old on your system - pyarrow 10.0.1 is the current minimum supported version as of this release.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "C:\\Sapna\\Nixtla_Project\\neuralforecast\\neuralforecast\\utils.py:281: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  AirPassengersPanel[\"y_[lag12]\"].fillna(AirPassengersPanel[\"y\"], inplace=True)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TSMixer' from 'neuralforecast.models.tsmixer' (C:\\Sapna\\Nixtla_Project\\neuralforecast\\neuralforecast\\models\\tsmixer\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuralforecast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeuralForecast\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuralforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AirPassengersPanel, AirPassengersStatic\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuralforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MAE\n",
      "File \u001b[1;32mC:\\Sapna\\Nixtla_Project\\neuralforecast\\neuralforecast\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.6.4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeuralForecast\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeuralForecast\n",
      "File \u001b[1;32mC:\\Sapna\\Nixtla_Project\\neuralforecast\\neuralforecast\\core.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutilsforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_freq\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesDataset\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuralforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     GRU,\n\u001b[0;32m     30\u001b[0m     LSTM,\n\u001b[0;32m     31\u001b[0m     RNN,\n\u001b[0;32m     32\u001b[0m     TCN,\n\u001b[0;32m     33\u001b[0m     DeepAR,\n\u001b[0;32m     34\u001b[0m     DilatedRNN,\n\u001b[0;32m     35\u001b[0m     MLP,\n\u001b[0;32m     36\u001b[0m     NHITS,\n\u001b[0;32m     37\u001b[0m     NBEATS,\n\u001b[0;32m     38\u001b[0m     NBEATSx,\n\u001b[0;32m     39\u001b[0m     DLinear,\n\u001b[0;32m     40\u001b[0m     TFT,\n\u001b[0;32m     41\u001b[0m     VanillaTransformer,\n\u001b[0;32m     42\u001b[0m     Informer,\n\u001b[0;32m     43\u001b[0m     Autoformer,\n\u001b[0;32m     44\u001b[0m     FEDformer,\n\u001b[0;32m     45\u001b[0m     StemGNN,\n\u001b[0;32m     46\u001b[0m     PatchTST,\n\u001b[0;32m     47\u001b[0m     TimesNet,\n\u001b[0;32m     48\u001b[0m     TSMixer,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# %% ../nbs/core.ipynb 5\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_insample_times\u001b[39m(\n\u001b[0;32m     53\u001b[0m     times: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m     54\u001b[0m     uids: Series,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     time_col: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     61\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n",
      "File \u001b[1;32mC:\\Sapna\\Nixtla_Project\\neuralforecast\\neuralforecast\\models\\__init__.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HINT\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtimesnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimesNet\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsmixer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSMixer\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TSMixer' from 'neuralforecast.models.tsmixer' (C:\\Sapna\\Nixtla_Project\\neuralforecast\\neuralforecast\\models\\tsmixer\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import Informer, Autoformer, FEDformer, PatchTST, TSMixer\n",
    "\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "Y_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6e694-e9f9-4fa9-892e-a989f69fb7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "model = TSMixer(h=12,\n",
    "                input_size=12,\n",
    "                n_series=2,\n",
    "                # stat_exog_list=['airline1'],\n",
    "                # futr_exog_list=['trend'],\n",
    "                scaler_type='robust',\n",
    "                max_steps=100,\n",
    "                early_stop_patience_steps=-1,\n",
    "                val_check_steps=10,\n",
    "                learning_rate=1e-3,\n",
    "                loss=MAE(),\n",
    "                # valid_loss=None,\n",
    "                batch_size=32,\n",
    "                n_block=1,\n",
    "                ff_dim=32,\n",
    "                target_slice=slice(0,2)\n",
    "                )\n",
    "\n",
    "fcst = NeuralForecast(models=[model], freq='ME')\n",
    "fcst.fit(df=Y_train_df, val_size=12)\n",
    "\n",
    "forecasts = fcst.predict(futr_df=Y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a1aea-c046-42bb-945d-32a5b4106226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Plot quantile predictions\n",
    "Y_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['TSMixer'], c='blue', label='TSMixer-Forecast')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c564e-f3a7-4547-80dc-4e28b5b4bb86",
   "metadata": {},
   "source": [
    "Using `cross_validation` to forecast multiple historic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e070809-02c7-4e6b-8e44-4b599ef0d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time = len(AirPassengersPanel.ds.unique())\n",
    "val_size = int(.2 * n_time)\n",
    "test_size = int(.2 * n_time)\n",
    "val_size=val_size\n",
    "test_size=test_size\n",
    "print(val_size,test_size)\n",
    "print(len(AirPassengersPanel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c170d-ffca-4a9a-92d8-ca9050441277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "fcst = NeuralForecast(models=[model], freq='ME')\n",
    "forecasts = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=2, step_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb6670-3837-49fc-9cef-88420c194334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Plot quantile predictions\n",
    "forecasts = pd.DataFrame(forecasts)\n",
    "forecasts.index\n",
    "\n",
    "Y_hat_df = forecasts[forecasts.index=='Airline1']\n",
    "Y_df = AirPassengersPanel[AirPassengersPanel['unique_id']=='Airline1']\n",
    "\n",
    "plt.plot(Y_df['ds'], Y_df['y'], c='black', label='True')\n",
    "plt.plot(Y_hat_df['ds'], Y_hat_df['TSMixer'], c='blue', label='TSMixer Forecast')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe7f07-6108-4e06-bc19-d907af4925d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
